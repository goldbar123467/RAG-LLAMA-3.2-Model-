# ğŸ§  MyAgent â€” Building and Merging LoRA Models from Scratch

**Author:** [Clark Kitchen](https://www.linkedin.com/in/clarkkitchen)  
**Repository:** [github.com/clarkkitchen22/my_agent](https://github.com/clarkkitchen22/my_agent)  
**Hugging Face Model:** [mistral-7b-lora-merged](https://huggingface.co/clarkkitchen22/mistral-7b-lora-merged)

---

## ğŸš€ Overview

**MyAgent** is a personal AI development project built entirely over a weekend â€” combining Python, Hugging Face tools, and local GPU workflows to **merge and deploy custom LoRA fine-tuned models**.  

The goal was to automate the process of merging lightweight adapters, testing inference locally, and publishing to the Hugging Face Hub. This project taught me how to move from idea â†’ working AI toolchain â€” covering dataset management, model compatibility, and deployment automation.

---

## ğŸ§© Key Features

- ğŸ”— **LoRA Model Merging** â€” Combines multiple LoRA adapters into a single unified model.  
- âš™ï¸ **Adapter Offload Management** â€” Handles `meta` and offloaded layers seamlessly.  
- ğŸ§° **One-Click Upload** â€” Automated push to Hugging Face with `huggingface_hub`.  
- ğŸ§® **GPU-Optimized** â€” Built and tested locally on RTX hardware (PyTorch + CUDA).  
- ğŸ’¾ **Safe Checkpointing** â€” Creates intermediate backups for rollback and testing.  
- ğŸ“Š **Readable, Modular Code** â€” Each component split into logical Python scripts.  

---

## ğŸ§  Technical Stack

| Component | Description |
|------------|--------------|
| **Language** | Python 3.12 |
| **Frameworks** | PyTorch, Transformers, PEFT |
| **Tools** | `huggingface_hub`, `accelerate`, `safetensors` |
| **Hardware** | NVIDIA RTX 2070 (CUDA 11.8) |
| **OS/Env** | Windows 10 + PowerShell (venv) |

---

## âš¡ Quick Start

Clone the repo and set up a virtual environment:
```bash
git clone https://github.com/clarkkitchen22/my_agent.git
cd my_agent
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
